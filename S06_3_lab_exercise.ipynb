{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc524e1",
   "metadata": {},
   "source": [
    "# NLP Practical Exam — Text Processing + Language Modeling (90 minutes)\n",
    "\n",
    "**Instructions**\n",
    "- Work in this notebook only.\n",
    "- Write short, clear comments to justify *tool choices* (regex vs NLTK, etc.).\n",
    "- Do **not** use external NLP libraries beyond **NLTK**, **NumPy**, **PyTorch** (PyTorch not needed here).\n",
    "- Keep outputs readable (print key variables).\n",
    "\n",
    "**Total: 10 points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16ffbb",
   "metadata": {},
   "source": [
    "## Given text\n",
    "\n",
    "```python\n",
    "text = (\"In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. A report valued the project at $3.2 billion.\")\n",
    "```\n",
    "\n",
    "> Treat the text as *synthetic exam data* (no fact-checking needed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f16d8e",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. **(1 pt)** Sentence splitting using **regex + NLTK**.\n",
    "2. **(1 pt)** Regex normalization: acronyms, height meters→centimeters, money `$X.Y billion` → `x point y billion` (words).\n",
    "3. **(1 pt)** Lowercase **except** proper nouns; join multiword proper nouns with underscore (e.g., `Sam Altman → Sam_Altman`). Keep acronyms uppercase.\n",
    "4. **(1 pt)** Tokenize (tool of your choice).\n",
    "5. **(1 pt)** Remove stopwords (tool of your choice); keep entity tokens.\n",
    "6. **(1 pt)** Create bigrams with pure Python.\n",
    "7. **(2 pt)** Build a bigram LM (MLE) and `predict_next(prev_word, top_k=3)`.\n",
    "\n",
    "8. **(2 pt)** Implement a simple **BPE** on: `corpus = \"low lower newest widest\"` (≥5 merges or until no merges).\n",
    "9. **(1 pt)** Compute Accuracy/Precision/Recall/F1 for an invented confusion matrix (explain with comments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f7ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. A report valued the project at $3.2 billion.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NLTK downloads (safe to run multiple times)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = (\"In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. \"\n",
    "        \"He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. \"\n",
    "        \"A report valued the project at $3.2 billion.\")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c9bdf",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2182c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona.\n",
      "Sentence 2: He is 1.86m tall and met with researchers from U.P.C.\n",
      "Sentence 3: and U.N.E.S.C.O.\n",
      "Sentence 4: A report valued the project at $3.2 billion.\n"
     ]
    }
   ],
   "source": [
    "# Q1 (1 pt): Sentence splitting (regex + NLTK)\n",
    "# - Use regex to protect acronyms like U.P.C. so they don't break sentence boundaries.\n",
    "# - Then use nltk.sent_tokenize.\n",
    "#\n",
    "# Return: sentences (list of strings)\n",
    "\n",
    "# TODO: implement protect_acronym_dots and restore_acronym_dots (or equivalent)\n",
    "# TODO: apply sent_tokenize\n",
    "sentences = None\n",
    "\n",
    "# print(sentences)\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def protect_acronym_dots(raw_text):\n",
    "    # This regex looks for a dot preceded by an uppercase letter \n",
    "    # and followed by another uppercase letter. \n",
    "    # It replaces that dot with a placeholder like \"<DOT>\".\n",
    "    return re.sub(r'(?<=[A-Z])\\.(?=[A-Z])', '<DOT>', raw_text)\n",
    "\n",
    "def restore_acronym_dots(sentence_list):\n",
    "    # This goes through the list of sentences and changes the placeholder back to a dot\n",
    "    return [sentence.replace('<DOT>', '.') for sentence in sentence_list]\n",
    "\n",
    "# Hide the internal acronym dots\n",
    "protected_text = protect_acronym_dots(text)\n",
    "\n",
    "# Split into sentences using NLTK\n",
    "raw_sentences = sent_tokenize(protected_text)\n",
    "\n",
    "# Bring the dots back\n",
    "sentences = restore_acronym_dots(raw_sentences)\n",
    "\n",
    "# Check the result\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"Sentence {i+1}: {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0cb6c",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec76b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 186 centimeters tall and met with researchers from UPC and UNESCO A report valued the project at three point two billion.\n"
     ]
    }
   ],
   "source": [
    "# Q2 (1 pt): Regex normalization\n",
    "# Convert:\n",
    "#  - U.P.C. -> UPC, U.N.E.S.C.O. -> UNESCO (general rule: remove dots in acronyms)\n",
    "#  - 1.86m -> 186 centimeters (general: X.YZm -> int(round(float(X.YZ)*100)) centimeters)\n",
    "#  - $3.2 billion -> three point two billion  (digits 0-9 are enough)\n",
    "#\n",
    "# Return: text_norm\n",
    "\n",
    "text_norm = None\n",
    "\n",
    "# print(text_norm)\n",
    "\n",
    "import re\n",
    "\n",
    "text_norm = text\n",
    "\n",
    "# 1. Acronyms: U.P.C. -> UPC\n",
    "# The regex looks for one or more uppercase letters followed by a dot.\n",
    "# replace the dots with nothing in the matched string.\n",
    "text_norm = re.sub(r'([A-Z]\\.)+', lambda match: match.group(0).replace('.', ''), text_norm)\n",
    "\n",
    "# 2. Height: 1.86m -> 186 centimeters\n",
    "# Capture the decimal number in group 1, convert it to float, \n",
    "# multiply by 100, round it, and turn it into an integer.\n",
    "text_norm = re.sub(r'(\\d+\\.\\d+)m\\b', \n",
    "                   lambda match: f\"{int(round(float(match.group(1)) * 100))} centimeters\", \n",
    "                   text_norm)\n",
    "\n",
    "# 3. Money: $3.2 billion -> three point two billion\n",
    "# Create a simple dictionary to map the digits.\n",
    "num_to_word = {\n",
    "    '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "    '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
    "}\n",
    "\n",
    "# Capture the two digits separately and replace them using our dictionary.\n",
    "def replace_money(match):\n",
    "    digit1 = num_to_word[match.group(1)]\n",
    "    digit2 = num_to_word[match.group(2)]\n",
    "    return f\"{digit1} point {digit2} billion\"\n",
    "\n",
    "text_norm = re.sub(r'\\$(\\d)\\.(\\d)\\s+billion', replace_money, text_norm)\n",
    "\n",
    "print(text_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbaf89",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c5a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in mid-February 2026, the CEO of OpenAI, Sam_Altman, visited Barcelona. he is 186 centimeters tall and met with researchers from UPC and UNESCO A report valued the project at three point two billion.\n"
     ]
    }
   ],
   "source": [
    "# Q3 (1 pt): Lowercase except proper nouns + underscore multiword proper nouns\n",
    "# Requirements:\n",
    "# - Convert to lowercase except:\n",
    "#   - Acronyms (ALL CAPS) stay uppercase (e.g., UNESCO, UPC, CEO)\n",
    "#   - MixedCase tokens stay as-is (e.g., OpenAI)\n",
    "#   - Multiword proper nouns joined with underscore (Sam Altman -> Sam_Altman) and preserved\n",
    "#\n",
    "# Return: text_case\n",
    "\n",
    "text_case = None\n",
    "\n",
    "# print(text_case)\n",
    "\n",
    "import re\n",
    "\n",
    "# Assuming text_norm is coming from the previous Q2 solution\n",
    "# text_norm = \"In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 186 centimeters tall and met with researchers from UPC and UNESCO. A report valued the project at three point two billion.\"\n",
    "\n",
    "text_case = text_norm\n",
    "\n",
    "# 1. Multiword proper nouns (e.g., \"Sam Altman\" -> \"Sam_Altman\")\n",
    "# Look for two or more consecutive capitalized words and replace the spaces with underscores.\n",
    "text_case = re.sub(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\b', \n",
    "                   lambda m: m.group(1).replace(' ', '_'), \n",
    "                   text_case)\n",
    "\n",
    "# 2. Lowercase everything else except the specific rules\n",
    "def smart_lowercase(match):\n",
    "    word = match.group(0)\n",
    "    \n",
    "    # Rule A: Acronyms (ALL CAPS). \n",
    "    # (Use len > 1 so we don't accidentally keep a starting 'A' uppercase)\n",
    "    if word.isupper() and len(word) > 1:\n",
    "        return word\n",
    "        \n",
    "    # Rule B: MixedCase (e.g., OpenAI). \n",
    "    # Check if there's any uppercase letter after the very first character.\n",
    "    if any(char.isupper() for char in word[1:]):\n",
    "        return word\n",
    "        \n",
    "    # Rule C: Multiword proper nouns.\n",
    "    # Since already added the underscore in step 1, I just check for it.\n",
    "    if '_' in word:\n",
    "        return word\n",
    "        \n",
    "    # Rule D: Single proper nouns (like \"Barcelona\" or \"February\").\n",
    "    # If it is capitalized, need to check if it's at the start of a sentence.\n",
    "    if word.istitle():\n",
    "        # Get the text right before this word to see where I am\n",
    "        text_before = text_case[:match.start()].strip()\n",
    "        \n",
    "        # If there's no text before, or the text before ends with a dot, \n",
    "        # it means I am at the beginning of a sentence. So, we lowercase it.\n",
    "        if not text_before or text_before.endswith('.'):\n",
    "            return word.lower() # \"In\", \"He\", \"A\" become \"in\", \"he\", \"a\"\n",
    "        else:\n",
    "            return word\n",
    "            \n",
    "    # For absolutely everything else, just return the lowercase version\n",
    "    return word.lower()\n",
    "\n",
    "# Apply to every word (letters and underscores)\n",
    "text_case = re.sub(r'[a-zA-Z_]+', smart_lowercase, text_case)\n",
    "\n",
    "print(text_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eaf848",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd9c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'mid-February', '2026', ',', 'the', 'CEO', 'of', 'OpenAI', ',', 'Sam_Altman', ',', 'visited', 'Barcelona', '.', 'he', 'is', '186', 'centimeters', 'tall', 'and', 'met', 'with', 'researchers', 'from', 'UPC', 'and', 'UNESCO', 'A', 'report', 'valued', 'the', 'project', 'at', 'three', 'point', 'two', 'billion', '.']\n"
     ]
    }
   ],
   "source": [
    "# Q4 (1 pt): Tokenization\n",
    "# Use a tokenizer of your choice (e.g., nltk.word_tokenize).\n",
    "# Return: tokens (list)\n",
    "\n",
    "tokens = None\n",
    "\n",
    "# print(tokens)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assuming text_case contains the string from the end of Q3\n",
    "# text_case = \"in mid-February 2026, the CEO of OpenAI, Sam_Altman, visited Barcelona. he is 186 centimeters tall and met with researchers from UPC and UNESCO. a report valued the project at three point two billion.\"\n",
    "\n",
    "# Use NLTK's word_tokenize to split the string into a list of tokens\n",
    "tokens = word_tokenize(text_case)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4e851",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 (1 pt): Stopword removal\n",
    "# - Remove English stopwords\n",
    "# - Do NOT remove entity tokens like OpenAI, Sam_Altman, Barcelona, UNESCO, UPC\n",
    "# Return: tokens_nostop\n",
    "\n",
    "tokens_nostop = None\n",
    "\n",
    "# print(tokens_nostop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab209a7",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94862b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 (1 pt): Bigrams with pure Python (no NLTK bigrams helper)\n",
    "# Return: bigrams = [(w1, w2), ...]\n",
    "\n",
    "bigrams = None\n",
    "\n",
    "# print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885728fc",
   "metadata": {},
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fab00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 (2 pt): Bigram Language Model + next-word prediction\n",
    "# Build:\n",
    "# - bigram_counts[(w1,w2)]\n",
    "# - context_counts[w1]\n",
    "# - model[w1][w2] = P(w2|w1) = count(w1,w2)/count(w1)\n",
    "#\n",
    "# Then implement:\n",
    "# def predict_next(prev_word, model, top_k=3): -> list[(next_word, prob)] sorted\n",
    "\n",
    "bigram_counts = None\n",
    "context_counts = None\n",
    "model = None\n",
    "\n",
    "def predict_next(prev_word, model, top_k=3):\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "# Example:\n",
    "# print(predict_next(\"OpenAI\", model, top_k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc35060",
   "metadata": {},
   "source": [
    "## Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 (2 pt): Simple BPE (Byte Pair Encoding) on a tiny corpus\n",
    "corpus = \"low lower newest widest\"\n",
    "\n",
    "# Requirements:\n",
    "# - Represent each word as characters + </w>\n",
    "# - Compute pair frequencies (weighted by word frequency)\n",
    "# - Merge most frequent pair\n",
    "# - Do at least 5 merges (or stop if no pairs)\n",
    "#\n",
    "# Deliver:\n",
    "# - merges: list of merges in order\n",
    "# - final segmented version of each word\n",
    "\n",
    "merges = None\n",
    "\n",
    "# TODO: implement BPE helper functions:\n",
    "# - get_vocab_from_corpus\n",
    "# - get_pair_frequencies\n",
    "# - merge_pair_in_vocab\n",
    "\n",
    "# print(merges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d34fa",
   "metadata": {},
   "source": [
    "## Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c22e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 (1 pt): Metrics — Accuracy, Precision, Recall, F1\n",
    "# Invent a confusion matrix (TP, FP, FN, TN) and compute metrics.\n",
    "# Explain each formula briefly in comments.\n",
    "\n",
    "TP = None\n",
    "FP = None\n",
    "FN = None\n",
    "TN = None\n",
    "\n",
    "accuracy = None\n",
    "precision = None\n",
    "recall = None\n",
    "f1 = None\n",
    "\n",
    "# print(accuracy, precision, recall, f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
