{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc524e1",
   "metadata": {},
   "source": [
    "# NLP Practical Exam — Text Processing + Language Modeling (90 minutes)\n",
    "\n",
    "**Instructions**\n",
    "- Work in this notebook only.\n",
    "- Write short, clear comments to justify *tool choices* (regex vs NLTK, etc.).\n",
    "- Do **not** use external NLP libraries beyond **NLTK**, **NumPy**, **PyTorch** (PyTorch not needed here).\n",
    "- Keep outputs readable (print key variables).\n",
    "\n",
    "**Total: 10 points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16ffbb",
   "metadata": {},
   "source": [
    "## Given text\n",
    "\n",
    "```python\n",
    "text = (\"In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. A report valued the project at $3.2 billion.\")\n",
    "```\n",
    "\n",
    "> Treat the text as *synthetic exam data* (no fact-checking needed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f16d8e",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. **(1 pt)** Sentence splitting using **regex + NLTK**.\n",
    "2. **(1 pt)** Regex normalization: acronyms, height meters→centimeters, money `$X.Y billion` → `x point y billion` (words).\n",
    "3. **(1 pt)** Lowercase **except** proper nouns; join multiword proper nouns with underscore (e.g., `Sam Altman → Sam_Altman`). Keep acronyms uppercase.\n",
    "4. **(1 pt)** Tokenize (tool of your choice).\n",
    "5. **(1 pt)** Remove stopwords (tool of your choice); keep entity tokens.\n",
    "6. **(1 pt)** Create bigrams with pure Python.\n",
    "7. **(2 pt)** Build a bigram LM (MLE) and `predict_next(prev_word, top_k=3)`.\n",
    "\n",
    "8. **(2 pt)** Implement a simple **BPE** on: `corpus = \"low lower newest widest\"` (≥5 merges or until no merges).\n",
    "9. **(1 pt)** Compute Accuracy/Precision/Recall/F1 for an invented confusion matrix (explain with comments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f7ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. A report valued the project at $3.2 billion.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NLTK downloads (safe to run multiple times)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = (\"In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona. \"\n",
    "        \"He is 1.86m tall and met with researchers from U.P.C. and U.N.E.S.C.O. \"\n",
    "        \"A report valued the project at $3.2 billion.\")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c9bdf",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2182c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: In mid-February 2026, the CEO of OpenAI, Sam Altman, visited Barcelona.\n",
      "Sentence 2: He is 1.86m tall and met with researchers from U.P.C.\n",
      "Sentence 3: and U.N.E.S.C.O.\n",
      "Sentence 4: A report valued the project at $3.2 billion.\n"
     ]
    }
   ],
   "source": [
    "# Q1 (1 pt): Sentence splitting (regex + NLTK)\n",
    "# - Use regex to protect acronyms like U.P.C. so they don't break sentence boundaries.\n",
    "# - Then use nltk.sent_tokenize.\n",
    "#\n",
    "# Return: sentences (list of strings)\n",
    "\n",
    "# TODO: implement protect_acronym_dots and restore_acronym_dots (or equivalent)\n",
    "# TODO: apply sent_tokenize\n",
    "sentences = None\n",
    "\n",
    "# print(sentences)\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def protect_acronym_dots(raw_text):\n",
    "    # This regex looks for a dot preceded by an uppercase letter \n",
    "    # and followed by another uppercase letter. \n",
    "    # It replaces that dot with a placeholder like \"<DOT>\".\n",
    "    return re.sub(r'(?<=[A-Z])\\.(?=[A-Z])', '<DOT>', raw_text)\n",
    "\n",
    "def restore_acronym_dots(sentence_list):\n",
    "    # This goes through the list of sentences and changes the placeholder back to a dot\n",
    "    return [sentence.replace('<DOT>', '.') for sentence in sentence_list]\n",
    "\n",
    "# Hide the internal acronym dots\n",
    "protected_text = protect_acronym_dots(text)\n",
    "\n",
    "# Split into sentences using NLTK\n",
    "raw_sentences = sent_tokenize(protected_text)\n",
    "\n",
    "# Bring the dots back\n",
    "sentences = restore_acronym_dots(raw_sentences)\n",
    "\n",
    "# Check the result\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"Sentence {i+1}: {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0cb6c",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 (1 pt): Regex normalization\n",
    "# Convert:\n",
    "#  - U.P.C. -> UPC, U.N.E.S.C.O. -> UNESCO (general rule: remove dots in acronyms)\n",
    "#  - 1.86m -> 186 centimeters (general: X.YZm -> int(round(float(X.YZ)*100)) centimeters)\n",
    "#  - $3.2 billion -> three point two billion  (digits 0-9 are enough)\n",
    "#\n",
    "# Return: text_norm\n",
    "\n",
    "text_norm = None\n",
    "\n",
    "# print(text_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbaf89",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 (1 pt): Lowercase except proper nouns + underscore multiword proper nouns\n",
    "# Requirements:\n",
    "# - Convert to lowercase except:\n",
    "#   - Acronyms (ALL CAPS) stay uppercase (e.g., UNESCO, UPC, CEO)\n",
    "#   - MixedCase tokens stay as-is (e.g., OpenAI)\n",
    "#   - Multiword proper nouns joined with underscore (Sam Altman -> Sam_Altman) and preserved\n",
    "#\n",
    "# Return: text_case\n",
    "\n",
    "text_case = None\n",
    "\n",
    "# print(text_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eaf848",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 (1 pt): Tokenization\n",
    "# Use a tokenizer of your choice (e.g., nltk.word_tokenize).\n",
    "# Return: tokens (list)\n",
    "\n",
    "tokens = None\n",
    "\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4e851",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 (1 pt): Stopword removal\n",
    "# - Remove English stopwords\n",
    "# - Do NOT remove entity tokens like OpenAI, Sam_Altman, Barcelona, UNESCO, UPC\n",
    "# Return: tokens_nostop\n",
    "\n",
    "tokens_nostop = None\n",
    "\n",
    "# print(tokens_nostop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab209a7",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94862b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 (1 pt): Bigrams with pure Python (no NLTK bigrams helper)\n",
    "# Return: bigrams = [(w1, w2), ...]\n",
    "\n",
    "bigrams = None\n",
    "\n",
    "# print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885728fc",
   "metadata": {},
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fab00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 (2 pt): Bigram Language Model + next-word prediction\n",
    "# Build:\n",
    "# - bigram_counts[(w1,w2)]\n",
    "# - context_counts[w1]\n",
    "# - model[w1][w2] = P(w2|w1) = count(w1,w2)/count(w1)\n",
    "#\n",
    "# Then implement:\n",
    "# def predict_next(prev_word, model, top_k=3): -> list[(next_word, prob)] sorted\n",
    "\n",
    "bigram_counts = None\n",
    "context_counts = None\n",
    "model = None\n",
    "\n",
    "def predict_next(prev_word, model, top_k=3):\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "# Example:\n",
    "# print(predict_next(\"OpenAI\", model, top_k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc35060",
   "metadata": {},
   "source": [
    "## Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 (2 pt): Simple BPE (Byte Pair Encoding) on a tiny corpus\n",
    "corpus = \"low lower newest widest\"\n",
    "\n",
    "# Requirements:\n",
    "# - Represent each word as characters + </w>\n",
    "# - Compute pair frequencies (weighted by word frequency)\n",
    "# - Merge most frequent pair\n",
    "# - Do at least 5 merges (or stop if no pairs)\n",
    "#\n",
    "# Deliver:\n",
    "# - merges: list of merges in order\n",
    "# - final segmented version of each word\n",
    "\n",
    "merges = None\n",
    "\n",
    "# TODO: implement BPE helper functions:\n",
    "# - get_vocab_from_corpus\n",
    "# - get_pair_frequencies\n",
    "# - merge_pair_in_vocab\n",
    "\n",
    "# print(merges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d34fa",
   "metadata": {},
   "source": [
    "## Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c22e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 (1 pt): Metrics — Accuracy, Precision, Recall, F1\n",
    "# Invent a confusion matrix (TP, FP, FN, TN) and compute metrics.\n",
    "# Explain each formula briefly in comments.\n",
    "\n",
    "TP = None\n",
    "FP = None\n",
    "FN = None\n",
    "TN = None\n",
    "\n",
    "accuracy = None\n",
    "precision = None\n",
    "recall = None\n",
    "f1 = None\n",
    "\n",
    "# print(accuracy, precision, recall, f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
