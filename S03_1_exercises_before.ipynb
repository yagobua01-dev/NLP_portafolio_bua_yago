{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting text into sentences and words when dealing with complex real-world text containing dates, amounts, URLs, emails, acronyms, and multi-word expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a text variable, split it into:\n",
    "1. **Sentences** - logical units of meaning ending with terminal punctuation\n",
    "2. **Words (tokens)** - individual meaningful units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n",
      "Sentences:\n",
      "- Dr.\n",
      "- John Smith, Ph.D., earned $1,250.50 on Jan.\n",
      "- 15, 2024, for his work at A.I.\n",
      "- Corp.\n",
      "- You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.\n",
      "- The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n"
     ]
    }
   ],
   "source": [
    "# Sample text with challenging elements\n",
    "import re\n",
    "text = \"\"\"Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for s in sentences:\n",
    "    print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "['Dr', 'John', 'Smith', 'Ph', 'D', 'earned', '1', '250', '50', 'on', 'Jan', '15', '2024', 'for', 'his', 'work', 'at', 'A', 'I', 'Corp', 'You', 'can', 'reach', 'him', 'at', 'j', 'smith', 'ai', 'corp', 'co', 'uk', 'or', 'visit', 'https', 'www', 'ai', 'corp', 'co', 'uk', 'team', 'dr', 'smith', 'for', 'more', 'info', 'The', 'U', 'S', 'A', 'based', 'company', 'reported', 'a', '23', '5', 'increase', 'in', 'Q3', 'revenue', 'totaling', '2', '5M']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting words in large corpuses and find the most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words:\n",
      "smith 3\n",
      "a 3\n",
      "corp 3\n",
      "dr 2\n",
      "for 2\n",
      "at 2\n",
      "ai 2\n",
      "co 2\n",
      "uk 2\n",
      "john 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "text = \"\"\"Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp.\n",
    "You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.\n",
    "The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\"\"\"\n",
    "\n",
    "tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "word_counts = Counter(tokens)\n",
    "print(\"Most common words:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(word, count)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a file `shakes.txt` in the book folder. Find the words that are more common in Shakespeare's book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in Shakespeare:\n",
      "the 27843\n",
      "and 26847\n",
      "i 22538\n",
      "to 19883\n",
      "of 18307\n",
      "a 14800\n",
      "you 13928\n",
      "my 12489\n",
      "that 11563\n",
      "in 11183\n",
      "is 9808\n",
      "d 8961\n",
      "not 8760\n",
      "for 8358\n",
      "with 8066\n",
      "me 7777\n",
      "it 7750\n",
      "s 7734\n",
      "be 7147\n",
      "this 6900\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Leer archivo\n",
    "with open(\"shakes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenizar (dividir en palabras)\n",
    "tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Contar palabras\n",
    "word_counts = Counter(tokens)\n",
    "\n",
    "# Mostrar las más comunes\n",
    "print(\"Most common words in Shakespeare:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "    print(word, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
