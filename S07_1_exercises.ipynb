{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S07 - Word Embeddings & Neural Networks\n",
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1 (Easy)\n",
        "Load pre-trained Word2Vec embeddings and find similar words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Top 5 words similar to 'king':\n",
            "prince: 0.7682\n",
            "queen: 0.7508\n",
            "son: 0.7021\n",
            "brother: 0.6986\n",
            "monarch: 0.6978\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained word2vec (google-news-300 or glove-wiki-gigaword-100)\n",
        "model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Find top 5 most similar words to 'king'\n",
        "similar_words = model.most_similar(\"king\", topn=5)\n",
        "\n",
        "print(\"Top 5 words similar to 'king':\")\n",
        "for word, score in similar_words:\n",
        "    print(f\"{word}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2 (Easy)\n",
        "Perform word analogy: king - man + woman = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "king - man + woman:\n",
            "queen: 0.7699\n",
            "monarch: 0.6843\n",
            "throne: 0.6756\n",
            "daughter: 0.6595\n",
            "princess: 0.6521\n",
            "\n",
            "paris - france + spain:\n",
            "madrid: 0.8061\n",
            "aires: 0.7141\n",
            "buenos: 0.6975\n",
            "prohertrib: 0.6854\n",
            "rome: 0.6849\n"
          ]
        }
      ],
      "source": [
        "# Use the model to solve: king - man + woman = ?\n",
        "# Also try: paris - france + spain = ?\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load a pre-trained model (same as Exercise 1)\n",
        "model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Analogy: king - man + woman = ?\n",
        "result1 = model.most_similar(positive=[\"king\", \"woman\"],\n",
        "                              negative=[\"man\"],\n",
        "                              topn=5)\n",
        "\n",
        "print(\"king - man + woman:\")\n",
        "for word, score in result1:\n",
        "    print(f\"{word}: {score:.4f}\")\n",
        "\n",
        "# Another analogy: paris - france + spain = ?\n",
        "result2 = model.most_similar(positive=[\"paris\", \"spain\"],\n",
        "                              negative=[\"france\"],\n",
        "                              topn=5)\n",
        "\n",
        "print(\"\\nparis - france + spain:\")\n",
        "for word, score in result2:\n",
        "    print(f\"{word}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3 (Medium)\n",
        "Train your own Word2Vec model on a custom corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector size for 'cat': 50\n",
            "\n",
            "Top 3 words similar to 'cat':\n",
            "and: 0.1656\n",
            "in: 0.1538\n",
            "need: 0.1366\n",
            "\n",
            "Top 3 words similar to 'dog':\n",
            "park: 0.1901\n",
            "need: 0.0449\n",
            "chased: -0.0101\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = [\n",
        "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"dog\", \"ran\", \"in\", \"the\", \"park\"],\n",
        "    [\"cats\", \"and\", \"dogs\", \"are\", \"pets\"],\n",
        "    [\"the\", \"cat\", \"chased\", \"the\", \"dog\"],\n",
        "    [\"pets\", \"need\", \"food\", \"and\", \"water\"]\n",
        "]\n",
        "\n",
        "# Train Word2Vec model (vector_size=50, window=3, min_count=1)\n",
        "model = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=50,\n",
        "    window=3,\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    sg=0  # 0 = CBOW, 1 = Skip-gram\n",
        ")\n",
        "\n",
        "# Get word vector\n",
        "cat_vector = model.wv[\"cat\"]\n",
        "print(\"Vector size for 'cat':\", len(cat_vector))\n",
        "\n",
        "# Find similar words\n",
        "similar_to_cat = model.wv.most_similar(\"cat\", topn=3)\n",
        "print(\"\\nTop 3 words similar to 'cat':\")\n",
        "for word, score in similar_to_cat:\n",
        "    print(f\"{word}: {score:.4f}\")\n",
        "\n",
        "# Try another word\n",
        "similar_to_dog = model.wv.most_similar(\"dog\", topn=3)\n",
        "print(\"\\nTop 3 words similar to 'dog':\")\n",
        "for word, score in similar_to_dog:\n",
        "    print(f\"{word}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4 (Medium)\n",
        "Build a simple neural network for text classification using embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        # Define: embedding layer, linear layer\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Embed -> mean pooling -> classify\n",
        "        pass\n",
        "\n",
        "# Test with dummy data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5 (Hard)\n",
        "Implement the Skip-gram model from scratch (forward pass only).\n",
        "\n",
        "*Research: Skip-gram predicts context words given center word.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # Two embedding matrices: center and context\n",
        "        pass\n",
        "    \n",
        "    def forward(self, center, context):\n",
        "        # Return dot product scores\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
